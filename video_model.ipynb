{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a96f9f02",
   "metadata": {},
   "source": [
    "# Downloading the dataset\n",
    "\n",
    "We are using the Face Forensics++ (FF++) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dce6fbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jordan/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/sanikatiwarekar/deep-fake-detection-dfd-entire-original-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22.5G/22.5G [23:54<00:00, 16.8MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to dataset files: /Users/jordan/.cache/kagglehub/datasets/sanikatiwarekar/deep-fake-detection-dfd-entire-original-dataset/versions/1\n"
     ]
    }
   ],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"sanikatiwarekar/deep-fake-detection-dfd-entire-original-dataset\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34863886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CSV file found in dataset directory: /Users/jordan/.cache/kagglehub/datasets/sanikatiwarekar/deep-fake-detection-dfd-entire-original-dataset/versions/1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004440c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real folder mp4 count: 363\n",
      "Fake folder mp4 count: 3068\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "#store the paths to the real and fake images in an env file\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "path = os.getenv('path')\n",
    "fake_path = os.getenv('DEEPFAKE_PATH')\n",
    "\n",
    "def count_mp4_files(folder):\n",
    "    mp4_count = 0\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        mp4_count += sum(1 for f in files if f.endswith('.mp4'))\n",
    "    return mp4_count\n",
    "\n",
    "real_mp4_count = count_mp4_files(path)\n",
    "fake_mp4_count = count_mp4_files(fake_path)\n",
    "\n",
    "print(f'Real folder mp4 count: {real_mp4_count}')\n",
    "print(f'Fake folder mp4 count: {fake_mp4_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f14d1c8",
   "metadata": {},
   "source": [
    "Use the EfficientNet model for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56bcc278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "def get_efficientnet_feature_extractor():\n",
    "    \"\"\"\n",
    "    Returns an EfficientNetB0 model that outputs feature vectors instead of class predictions.\n",
    "    \"\"\"\n",
    "    # Load EfficientNetB0 without the top classification layer\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')\n",
    "    # The output will be the global average pooled features\n",
    "    feature_extractor = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "    return feature_extractor\n",
    "\n",
    "def extract_face_features(face_images, feature_extractor):\n",
    "    \"\"\"\n",
    "    Given a batch of face images, returns their EfficientNet feature vectors.\n",
    "    face_images: numpy array of shape (batch_size, height, width, channels)\n",
    "    feature_extractor: model returned by get_efficientnet_feature_extractor()\n",
    "    \"\"\"\n",
    "    # EfficientNet expects images scaled to [0, 255] and size 224x224\n",
    "    # You may need to preprocess your images accordingly\n",
    "    features = feature_extractor.predict(face_images)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "71ba8d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def save_video_sequence_as_npy(video_path, output_dir, yolo_model_path='yolov8n.pt', conf=0.5, save_images=False):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        return\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if save_images:\n",
    "        os.makedirs(os.path.join(output_dir, 'images'), exist_ok=True)\n",
    "    \n",
    "    model = YOLO(yolo_model_path)\n",
    "    frame_count = 0\n",
    "    video_sequence = []  # Store all frames in a list\n",
    "    \n",
    "    while frame_count < 210:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Could not read frame {frame_count}\")\n",
    "            break\n",
    "        \n",
    "        results = model.predict(source=frame, conf=conf, classes=0, verbose=False)\n",
    "        \n",
    "        # Find the largest face in this frame\n",
    "        largest_face = None\n",
    "        largest_area = 0\n",
    "        \n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                area = (x2 - x1) * (y2 - y1)\n",
    "                \n",
    "                if area > largest_area:\n",
    "                    largest_area = area\n",
    "                    largest_face = (x1, y1, x2, y2)\n",
    "        \n",
    "        # Process and add frame to sequence if face found\n",
    "        if largest_face:\n",
    "            x1, y1, x2, y2 = largest_face\n",
    "            face = frame[y1:y2, x1:x2]\n",
    "            face_resized = cv2.resize(face, (224, 224))\n",
    "            face_scaled = face_resized.astype(np.float32) / 127.5 - 1.0\n",
    "            \n",
    "            # Add to video sequence\n",
    "            video_sequence.append(face_scaled)\n",
    "            \n",
    "            # Optionally save individual images for visualization\n",
    "            if save_images:\n",
    "                face_image = ((face_scaled + 1.0) * 127.5).astype(np.uint8)\n",
    "                cv2.imwrite(os.path.join(output_dir, 'images', f\"face_{frame_count:04d}.png\"), face_image)\n",
    "        else:\n",
    "            print(f\"No face detected in frame {frame_count}\")\n",
    "            # You could optionally add a zero/placeholder frame here\n",
    "            # placeholder = np.zeros((224, 224, 3), dtype=np.float32)\n",
    "            # video_sequence.append(placeholder)\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Convert list to numpy array and save as single .npy file\n",
    "    if video_sequence:\n",
    "        video_array = np.array(video_sequence)  # Shape: (num_frames, 224, 224, 3)\n",
    "        \n",
    "        # Get video name for the .npy file\n",
    "        video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        npy_path = os.path.join(output_dir, f\"{video_name}.npy\")\n",
    "        \n",
    "        np.save(npy_path, video_array)\n",
    "        print(f\"Saved video sequence: {video_array.shape} to {npy_path}\")\n",
    "    else:\n",
    "        print(\"No faces detected in any frame - no .npy file saved\")\n",
    "\n",
    "\n",
    "def process_videos(path, name):\n",
    "    if not path or not os.path.exists(path):\n",
    "        print(\"Path not found or doesn't exist\")\n",
    "        return\n",
    "    \n",
    "    # Find all mp4 files in the folder\n",
    "    video_files = glob.glob(os.path.join(path, \"*.mp4\"))\n",
    "    \n",
    "    if not video_files:\n",
    "        print(\"No mp4 files found in folder\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(video_files)} videos to process\")\n",
    "    \n",
    "    # Create main output directory\n",
    "    output_base = f'processed_{name}_sequences'\n",
    "    os.makedirs(output_base, exist_ok=True)\n",
    "    \n",
    "    for i, video_path in enumerate(video_files):\n",
    "        video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        print(f\"Processing video {i+1}/{len(video_files)}: {video_name}\")\n",
    "        \n",
    "        try:\n",
    "            save_video_sequence_as_npy(video_path, output_base, save_images=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {video_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Finished processing all videos in {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c457dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Get the path to the fake videos folder from your .env\n",
    "# fake_path = os.getenv('DEEPFAKE_PATH')\n",
    "\n",
    "# # List all mp4 files in the fake folder\n",
    "# fake_videos = [f for f in os.listdir(fake_path) if f.endswith('.mp4')]\n",
    "\n",
    "# if fake_videos:\n",
    "#     first_video = fake_videos[0]\n",
    "#     video_path = os.path.join(fake_path, first_video)\n",
    "#     output_dir = 'example_output'  # Change this to your desired output folder\n",
    "\n",
    "#     print(f\"Processing video: {video_path}\")\n",
    "#     save_first_210_cropped_faces(video_path, output_dir, save_images=True)\n",
    "# else:\n",
    "#     print(\"No fake videos found in the folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4519c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# def process_videos(path, name):\n",
    "    \n",
    "#     if not path or not os.path.exists(path):\n",
    "#         print(\"Path not found or doesn't exist\")\n",
    "#         return\n",
    "    \n",
    "#     # Find all mp4 files in the real folder\n",
    "#     video_files = glob.glob(os.path.join(path, \"*.mp4\"))\n",
    "    \n",
    "#     if not video_files:\n",
    "#         print(\"No mp4 files found in folder\")\n",
    "#         return\n",
    "    \n",
    "#     print(f\"Found {len(video_files)} videos to process\")\n",
    "    \n",
    "#     for i, video_path in enumerate(video_files):\n",
    "#         # Get video filename without extension\n",
    "#         video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        \n",
    "#         # Create output directory for this video\n",
    "#         output_dir = os.path.join(f'processed_{name}_videos', video_name)\n",
    "        \n",
    "#         print(f\"Processing video {i+1}/{len(video_files)}: {video_name}\")\n",
    "        \n",
    "#         try:\n",
    "#             save_first_210_cropped_faces(video_path, output_dir, save_images=False)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {video_name}: {e}\")\n",
    "#             continue\n",
    "    \n",
    "#     print(f\"Finished processing all videos in {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "96de4048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: /Users/jordan/.cache/kagglehub/datasets/sanikatiwarekar/deep-fake-detection-dfd-entire-original-dataset/versions/1/DFD_manipulated_sequences/DFD_manipulated_sequences/13_20__walking_down_indoor_hall_disgust__EV1V4ZQV.mp4\n",
      "Output: processed_13_20__walking_down_indoor_hall_disgust__EV1V4ZQV_faces.mp4\n",
      "Frame 0: Face detected and saved\n",
      "Frame 1: Face detected and saved\n",
      "Frame 2: Face detected and saved\n",
      "Frame 3: Face detected and saved\n",
      "Frame 4: Face detected and saved\n",
      "Frame 5: Face detected and saved\n",
      "Frame 6: Face detected and saved\n",
      "Frame 7: Face detected and saved\n",
      "Frame 8: Face detected and saved\n",
      "Frame 9: Face detected and saved\n",
      "Frame 10: Face detected and saved\n",
      "Frame 11: Face detected and saved\n",
      "Frame 12: Face detected and saved\n",
      "Frame 13: Face detected and saved\n",
      "Frame 14: Face detected and saved\n",
      "Frame 15: Face detected and saved\n",
      "Frame 16: Face detected and saved\n",
      "Frame 17: Face detected and saved\n",
      "Frame 18: Face detected and saved\n",
      "Frame 19: Face detected and saved\n",
      "Frame 20: Face detected and saved\n",
      "Frame 21: Face detected and saved\n",
      "Frame 22: Face detected and saved\n",
      "Frame 23: Face detected and saved\n",
      "Frame 24: Face detected and saved\n",
      "Frame 25: Face detected and saved\n",
      "Frame 26: Face detected and saved\n",
      "Frame 27: Face detected and saved\n",
      "Frame 28: Face detected and saved\n",
      "Frame 29: Face detected and saved\n",
      "Frame 30: Face detected and saved\n",
      "Frame 31: Face detected and saved\n",
      "Frame 32: Face detected and saved\n",
      "Frame 33: Face detected and saved\n",
      "Frame 34: Face detected and saved\n",
      "Frame 35: Face detected and saved\n",
      "Frame 36: Face detected and saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     80\u001b[39m     video_name = os.path.splitext(first_video)[\u001b[32m0\u001b[39m]\n\u001b[32m     81\u001b[39m     output_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprocessed_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_faces.mp4\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[43mprocess_single_video_to_mp4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo fake videos found in the folder.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mprocess_single_video_to_mp4\u001b[39m\u001b[34m(video_path, output_path, yolo_model_path, conf)\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not read frame \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Find the largest face in this frame\u001b[39;00m\n\u001b[32m     38\u001b[39m largest_face = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/ultralytics/engine/model.py:557\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, source, stream, predictor, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.predictor, \u001b[33m\"\u001b[39m\u001b[33mset_prompts\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[32m    556\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor.set_prompts(prompts)\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictor.predict_cli(source=source) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/ultralytics/engine/predictor.py:229\u001b[39m, in \u001b[36mBasePredictor.__call__\u001b[39m\u001b[34m(self, source, model, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs)\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:38\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         response = \u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     41\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     42\u001b[39m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/ultralytics/engine/predictor.py:336\u001b[39m, in \u001b[36mBasePredictor.stream_inference\u001b[39m\u001b[34m(self, source, model, *args, **kwargs)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.embed:\n\u001b[32m    338\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/ultralytics/engine/predictor.py:184\u001b[39m, in \u001b[36mBasePredictor.inference\u001b[39m\u001b[34m(self, im, *args, **kwargs)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[32m    179\u001b[39m visualize = (\n\u001b[32m    180\u001b[39m     increment_path(\u001b[38;5;28mself\u001b[39m.save_dir / Path(\u001b[38;5;28mself\u001b[39m.batch[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).stem, mkdir=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.visualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source_type.tensor)\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    183\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/ultralytics/nn/autobackend.py:637\u001b[39m, in \u001b[36mAutoBackend.forward\u001b[39m\u001b[34m(self, im, augment, visualize, embed, **kwargs)\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[32m    636\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nn_module:\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/ultralytics/nn/tasks.py:139\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/ultralytics/nn/tasks.py:157\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/ultralytics/nn/tasks.py:180\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    181\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/ultralytics/nn/modules/block.py:317\u001b[39m, in \u001b[36mC2f.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m    316\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     y = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m.chunk(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m    318\u001b[39m     y.extend(m(y[-\u001b[32m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.m)\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv2(torch.cat(y, \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/ultralytics/nn/modules/conv.py:93\u001b[39m, in \u001b[36mConv.forward_fuse\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     84\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[32m     86\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     91\u001b[39m \u001b[33;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/torch/nn/modules/activation.py:434\u001b[39m, in \u001b[36mSiLU.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live_Projects/History_of_Man/proactives_venv/lib/python3.13/site-packages/torch/nn/functional.py:2374\u001b[39m, in \u001b[36msilu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   2372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(silu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace=inplace)\n\u001b[32m   2373\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m-> \u001b[39m\u001b[32m2374\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2375\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.silu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def process_single_video_to_mp4(video_path, output_path, yolo_model_path='yolov8n.pt', conf=0.5):\n",
    "    \"\"\"\n",
    "    Process a single video, crop faces, and save as MP4\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width, height = 224, 224  # Output resolution\n",
    "    \n",
    "    # Create video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    model = YOLO(yolo_model_path)\n",
    "    frame_count = 0\n",
    "    \n",
    "    print(f\"Processing video: {video_path}\")\n",
    "    print(f\"Output: {output_path}\")\n",
    "    \n",
    "    while frame_count < 210:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Could not read frame {frame_count}\")\n",
    "            break\n",
    "        \n",
    "        results = model.predict(source=frame, conf=conf, classes=0, verbose=False)\n",
    "        \n",
    "        # Find the largest face in this frame\n",
    "        largest_face = None\n",
    "        largest_area = 0\n",
    "        \n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                area = (x2 - x1) * (y2 - y1)\n",
    "                \n",
    "                if area > largest_area:\n",
    "                    largest_area = area\n",
    "                    largest_face = (x1, y1, x2, y2)\n",
    "        \n",
    "        # Process and write frame\n",
    "        if largest_face:\n",
    "            x1, y1, x2, y2 = largest_face\n",
    "            face = frame[y1:y2, x1:x2]\n",
    "            face_resized = cv2.resize(face, (224, 224))\n",
    "            \n",
    "            # Write the cropped face to output video\n",
    "            out.write(face_resized)\n",
    "            print(f\"Frame {frame_count}: Face detected and saved\")\n",
    "        else:\n",
    "            # Write a black frame if no face detected\n",
    "            black_frame = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "            out.write(black_frame)\n",
    "            print(f\"Frame {frame_count}: No face detected, black frame saved\")\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Finished processing. Output saved to: {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "fake_path = os.getenv('DEEPFAKE_PATH')\n",
    "fake_videos = [f for f in os.listdir(fake_path) if f.endswith('.mp4')]\n",
    "\n",
    "if fake_videos:\n",
    "    first_video = fake_videos[0]\n",
    "    video_path = os.path.join(fake_path, first_video)\n",
    "    \n",
    "    # Create output filename\n",
    "    video_name = os.path.splitext(first_video)[0]\n",
    "    output_path = f\"processed_{video_name}_faces.mp4\"\n",
    "    \n",
    "    process_single_video_to_mp4(video_path, output_path)\n",
    "else:\n",
    "    print(\"No fake videos found in the folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e44adfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 363 videos to process\n",
      "Processing video 1/363: 07__exit_phone_room\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/07__exit_phone_room.npy\n",
      "Processing video 2/363: 09__kitchen_pan\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/09__kitchen_pan.npy\n",
      "Processing video 3/363: 02__walking_down_street_outside_angry\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/02__walking_down_street_outside_angry.npy\n",
      "Processing video 4/363: 12__talking_angry_couch\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/12__talking_angry_couch.npy\n",
      "Processing video 5/363: 11__podium_speech_happy\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/11__podium_speech_happy.npy\n",
      "Processing video 6/363: 26__podium_speech_happy\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/26__podium_speech_happy.npy\n",
      "Processing video 7/363: 24__kitchen_still\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/24__kitchen_still.npy\n",
      "Processing video 8/363: 26__outside_talking_pan_laughing\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/26__outside_talking_pan_laughing.npy\n",
      "Processing video 9/363: 11__walking_down_street_outside_angry\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/11__walking_down_street_outside_angry.npy\n",
      "Processing video 10/363: 16__walk_down_hall_angry\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/16__walk_down_hall_angry.npy\n",
      "Processing video 11/363: 17__talking_against_wall\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/17__talking_against_wall.npy\n",
      "Processing video 12/363: 18__secret_conversation\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/18__secret_conversation.npy\n",
      "Processing video 13/363: 14__walking_and_outside_surprised\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/14__walking_and_outside_surprised.npy\n",
      "Processing video 14/363: 05__outside_talking_still_laughing\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/05__outside_talking_still_laughing.npy\n",
      "Processing video 15/363: 21__talking_angry_couch\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/21__talking_angry_couch.npy\n",
      "Processing video 16/363: 12__outside_talking_pan_laughing\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/12__outside_talking_pan_laughing.npy\n",
      "Processing video 17/363: 24__walking_down_street_outside_angry\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/24__walking_down_street_outside_angry.npy\n",
      "Processing video 18/363: 21__outside_talking_still_laughing\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/21__outside_talking_still_laughing.npy\n",
      "Processing video 19/363: 27__hugging_happy\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/27__hugging_happy.npy\n",
      "Processing video 20/363: 22__podium_speech_happy\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/22__podium_speech_happy.npy\n",
      "Processing video 21/363: 28__walking_outside_cafe_disgusted\n",
      "No face detected in frame 0\n",
      "No face detected in frame 1\n",
      "No face detected in frame 2\n",
      "No face detected in frame 3\n",
      "No face detected in frame 4\n",
      "No face detected in frame 5\n",
      "No face detected in frame 6\n",
      "No face detected in frame 7\n",
      "No face detected in frame 8\n",
      "No face detected in frame 9\n",
      "No face detected in frame 10\n",
      "No face detected in frame 11\n",
      "No face detected in frame 12\n",
      "No face detected in frame 13\n",
      "No face detected in frame 14\n",
      "No face detected in frame 15\n",
      "No face detected in frame 16\n",
      "No face detected in frame 17\n",
      "No face detected in frame 18\n",
      "No face detected in frame 20\n",
      "Saved video sequence: (190, 224, 224, 3) to processed_real_sequences/28__walking_outside_cafe_disgusted.npy\n",
      "Processing video 22/363: 02__exit_phone_room\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/02__exit_phone_room.npy\n",
      "Processing video 23/363: 15__podium_speech_happy\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/15__podium_speech_happy.npy\n",
      "Processing video 24/363: 03__outside_talking_pan_laughing\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/03__outside_talking_pan_laughing.npy\n",
      "Processing video 25/363: 06__walking_and_outside_surprised\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/06__walking_and_outside_surprised.npy\n",
      "Processing video 26/363: 28__kitchen_pan\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/28__kitchen_pan.npy\n",
      "Processing video 27/363: 16__kitchen_pan\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/16__kitchen_pan.npy\n",
      "Processing video 28/363: 27__walking_outside_cafe_disgusted\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/27__walking_outside_cafe_disgusted.npy\n",
      "Processing video 29/363: 19__podium_speech_happy\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/19__podium_speech_happy.npy\n",
      "Processing video 30/363: 25__walk_down_hall_angry\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/25__walk_down_hall_angry.npy\n",
      "Processing video 31/363: 27__walking_down_street_outside_angry\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/27__walking_down_street_outside_angry.npy\n",
      "Processing video 32/363: 24__talking_against_wall\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/24__talking_against_wall.npy\n",
      "Processing video 33/363: 04__kitchen_still\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/04__kitchen_still.npy\n",
      "Processing video 34/363: 02__kitchen_pan\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/02__kitchen_pan.npy\n",
      "Processing video 35/363: 19__outside_talking_still_laughing\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/19__outside_talking_still_laughing.npy\n",
      "Processing video 36/363: 10__walking_outside_cafe_disgusted\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/10__walking_outside_cafe_disgusted.npy\n",
      "Processing video 37/363: 17__hugging_happy\n",
      "No face detected in frame 178\n",
      "Saved video sequence: (209, 224, 224, 3) to processed_real_sequences/17__hugging_happy.npy\n",
      "Processing video 38/363: 22__walking_and_outside_surprised\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/22__walking_and_outside_surprised.npy\n",
      "Processing video 39/363: 14__kitchen_still\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/14__kitchen_still.npy\n",
      "Processing video 40/363: 01__walking_down_street_outside_angry\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/01__walking_down_street_outside_angry.npy\n",
      "Processing video 41/363: 19__kitchen_still\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/19__kitchen_still.npy\n",
      "Processing video 42/363: 23__kitchen_pan\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/23__kitchen_pan.npy\n",
      "Processing video 43/363: 23__secret_conversation\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/23__secret_conversation.npy\n",
      "Processing video 44/363: 08__exit_phone_room\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/08__exit_phone_room.npy\n",
      "Processing video 45/363: 14__secret_conversation\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/14__secret_conversation.npy\n",
      "Processing video 46/363: 03__walk_down_hall_angry\n",
      "Error processing 03__walk_down_hall_angry: [Errno 2] No such file or directory: 'processed_real_sequences/03__walk_down_hall_angry.npy'\n",
      "Processing video 47/363: 03__walking_outside_cafe_disgusted\n",
      "No face detected in frame 0\n",
      "No face detected in frame 1\n",
      "No face detected in frame 2\n",
      "No face detected in frame 3\n",
      "No face detected in frame 4\n",
      "No face detected in frame 5\n",
      "No face detected in frame 6\n",
      "No face detected in frame 7\n",
      "No face detected in frame 8\n",
      "No face detected in frame 9\n",
      "No face detected in frame 10\n",
      "No face detected in frame 11\n",
      "No face detected in frame 12\n",
      "No face detected in frame 13\n",
      "Saved video sequence: (196, 224, 224, 3) to processed_real_sequences/03__walking_outside_cafe_disgusted.npy\n",
      "Processing video 48/363: 02__talking_against_wall\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/02__talking_against_wall.npy\n",
      "Processing video 49/363: 07__hugging_happy\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/07__hugging_happy.npy\n",
      "Processing video 50/363: 12__walking_down_street_outside_angry\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/12__walking_down_street_outside_angry.npy\n",
      "Processing video 51/363: 19__kitchen_pan\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/19__kitchen_pan.npy\n",
      "Processing video 52/363: 27__kitchen_pan\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/27__kitchen_pan.npy\n",
      "Processing video 53/363: 22__exit_phone_room\n",
      "No face detected in frame 0\n",
      "No face detected in frame 1\n",
      "Saved video sequence: (208, 224, 224, 3) to processed_real_sequences/22__exit_phone_room.npy\n",
      "Processing video 54/363: 17__kitchen_still\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/17__kitchen_still.npy\n",
      "Processing video 55/363: 05__talking_against_wall\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/05__talking_against_wall.npy\n",
      "Processing video 56/363: 04__walk_down_hall_angry\n",
      "No face detected in frame 4\n",
      "Saved video sequence: (209, 224, 224, 3) to processed_real_sequences/04__walk_down_hall_angry.npy\n",
      "Processing video 57/363: 06__outside_talking_still_laughing\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/06__outside_talking_still_laughing.npy\n",
      "Processing video 58/363: 21__walking_down_street_outside_angry\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/21__walking_down_street_outside_angry.npy\n",
      "Processing video 59/363: 03__podium_speech_happy\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/03__podium_speech_happy.npy\n",
      "Processing video 60/363: 07__kitchen_still\n",
      "Saved video sequence: (210, 224, 224, 3) to processed_real_sequences/07__kitchen_still.npy\n",
      "Processing video 61/363: 22__outside_talking_still_laughing\n"
     ]
    }
   ],
   "source": [
    "real = os.getenv('REAL_PATH')\n",
    "fake = os.getenv('DEEPFAKE_PATH')\n",
    "\n",
    "process_videos(real, \"real\")\n",
    "process_videos(fake, \"fake\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proactives_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
